\documentclass{beamer}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}

\usetheme{AnnArbor}
\usecolortheme{crane}
\usefonttheme[onlymath]{serif}

\title{Deep Learning - Foundations and Concepts}
\subtitle{Chapter 6. Deep Neural Networks}
\author{nonlineark@github}
\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Limitations of Fixed Basis Functions}

\begin{frame}
    \frametitle{The curse of dimensionality}
    In spaces of higher dimensionality, the number of combinations of values must be considered could be huge. This effect is known as combinatorial explosion:
    \begin{itemize}
        \item A polynomial regression of order $M$ for a single input variable needs $M+1$ parameters. If there are $D$ input variables, the number of parameters needed will be $\binom{M+D}{M}$.
        \item The histogram based classification for $1$-dimensional input needs $N$ buckets. If the input is $D$-dimensional, the number of buckets needed will be $N^{D}$.
    \end{itemize}
    For a machine learning model, this usually means that the amount of data needed to generlize accurately grows exponentially.
\end{frame}

\begin{frame}
    \frametitle{High-dimensional spaces}
    High-dimensional spaces can defeat one's geometrical intuitions:
    \begin{itemize}
        \item In spaces of high dimensionality, most of the volume of a hypersphere is concentrated in a thin shell near the surface.
        \item In spaces of high dimensionality, the probability mass of the Gaussian is concentrated in a thin shell at a specific radius (a soap bubble).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Data manifolds}
    Although data may be in high-dimensional spaces, real data will generally be confined to a region of the data space having lower effective dimensionality. Effectively, neural networks learn a set of basis functions that are adpated to data manifolds.
\end{frame}

\begin{frame}
    \frametitle{Data manifolds}
    \begin{figure}
        \caption{Images of a handwritten digit that lives on a nonlinear three-dimensional manifold}
        \includegraphics{Figure_7.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Data manifolds}
    \begin{figure}
        \caption{Natural images vs. randomly generated images}
        \includegraphics[height=0.7\textheight]{Figure_8.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Data-dependent basis functions}
    \begin{itemize}
        \item Simple basis functions that are chosen independently of the problem being solved can run into significant limitations.
        \item Using expert knowledge to hand-craft the basis functions was superseded by data-driven approaches in which basis functions are learned from the training data.
        \item Methods such as radial basis functions and support vector machines have been superseded by deep neural networks, which are much better at exploiting very large data sets efficiently.
    \end{itemize}
\end{frame}

\section{Multipayer Networks}

\begin{frame}
    \frametitle{Parameter matrices}
    Consider a basic neural network model having two layers of learnable parameters:
    \begin{align*}
        a_{m}^{(1)}&=\sum_{d=1}^{D}w_{md}^{(1)}x_{d}+w_{m0}^{(1)} \\
        z_{m}^{(1)}&=h(a_{m}^{(1)}) \\
        a_{k}^{(2)}&=\sum_{m=1}^{M}w_{km}^{(2)}z_{m}^{(1)}+w_{k0}^{(2)}
    \end{align*}
    where $h$ is a differentiable, nonlinear activation function.
\end{frame}

\begin{frame}
    \frametitle{Parameter matrices}
    \begin{figure}
        \caption{Network diagram for a two-layer neural network}
        \includegraphics{Figure_9.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Parameter matrices}
    The bias parameters can be absorbed into the set of weight parameters, so the two-layer neural network can be represented as:
    \begin{align*}
        y_{k}(x;w)&=f(\sum_{m=0}^{M}w_{km}^{(2)}h(\sum_{d=0}^{D}w_{md}^{(1)}x_{d})) \\
        y(x;w)&=f(W^{(2)}h(W^{(1)}x))
    \end{align*}
    where $f$ and $h$ are activation functions evaluated on each vector element separately.
\end{frame}

\begin{frame}
    \frametitle{Universal approximation}
    \begin{itemize}
        \item For a wide range of activation functions, two-layer feed-forward networks can approximate any function defined over a continuous subset of $\mathbb{R}^{D}$ to arbitrary accuracy.
        \item However, in a practical application, there can be huge benefits in considering networks having many more than two layers that can learn hierarchical internal representations.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Universal approximation}
    \begin{figure}
        \caption{Two-layer neural networks are universal approximators}
        \begin{subfigure}{0.7\textwidth}
            \includegraphics[width=0.45\textwidth]{Figure_10_a.pdf}
            \includegraphics[width=0.45\textwidth]{Figure_10_b.pdf}
        \end{subfigure}
        \begin{subfigure}{0.7\textwidth}
            \includegraphics[width=0.45\textwidth]{Figure_10_c.pdf}
            \includegraphics[width=0.45\textwidth]{Figure_10_d.pdf}
        \end{subfigure}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Hidden unit activation functions}
    \begin{itemize}
        \item Activation functions for the output units are determined by the kind of distribution being modelled.
        \item For the hidden units, the only requirement is that they need to be differentiable.
        \item Obviously, the identity function, sometimes used as the activation function for output units, is not a good option for hidden units.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Hidden unit activation functions}
    \begin{figure}
        \caption{A variety of nonlinear activation functions}
        \begin{subfigure}{0.8\textwidth}
            \includegraphics[width=0.3\textwidth]{Figure_12_a.pdf}
            \includegraphics[width=0.3\textwidth]{Figure_12_b.pdf}
            \includegraphics[width=0.3\textwidth]{Figure_12_c.pdf}
        \end{subfigure}
        \begin{subfigure}{0.8\textwidth}
            \includegraphics[width=0.3\textwidth]{Figure_12_d.pdf}
            \includegraphics[width=0.3\textwidth]{Figure_12_e.pdf}
            \includegraphics[width=0.3\textwidth]{Figure_12_f.pdf}
        \end{subfigure}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Weight-space symmetries}
    Consider a two-layer network with $M$ hidden units having $\tanh$ activation functions and full connectivity in both layers:
    \begin{itemize}
        \item Changing the sign of all the weights and the bias feeding into a particular hidden unit can be compensated by changing the sign of all the weights leading out of that hidden unit:
        \begin{itemize}
            \item $2^{M}$ equivalent weight vectors.
        \end{itemize}
        \item Interchange a particular hidden unit with a different hidden unit:
        \begin{itemize}
            \item $M!$ equivalent weight vectors.
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Deep Networks}

\begin{frame}
    \frametitle{Deep networks}
    We can easily extend the two-layer network architecture to any finite number $L$ of layers:
    \begin{equation*}
        z^{(l)}=h^{(l)}(W^{(l)}z^{(l-1)})\qquad{}l=1,\hdots,L
    \end{equation*}
    where $h^{(l)}$ denotes the activation function associated with layer $l$, and $W^{(l)}$ denotes the corresponding matrix of weight and bias parameters.
\end{frame}

\begin{frame}
    \frametitle{Hierarchical representations}
    The deep neural network architecture encodes a particular form of inductive bias, namely the outputs are related to the input space through a hierarchical representation:
    \begin{itemize}
        \item Low-level features in early layers, e.g., edges.
        \item Higher-level features in subsequent layers, e.g., eyes.
        \item Combined in later layers to detect high-level concept, e.g., cats.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Distributed representations}
    Consider a hidden layer with $M$ hidden units:
    \begin{itemize}
        \item Conceptually, each hidden unit can be thought of as representing a feature, so this hidden layer can represent $M$ different features.
        \item However, the network can learn a different representation, in which combinations of hidden units represent features, so this hidden layer can represent $2^{M}$ different features.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Representation learning}
    Consider a neural network for a two-class classification problem:
    \begin{itemize}
        \item The final layer can be viewed as a simple linear classifier.
        \item In the representation of the last hidden layer, the two classes must be well separated by a linear surface.
        \item This neural network transforms the input data into a representation that's easy for classification purposes.
    \end{itemize}
    This ability to discover a nonlinear transformation of the data that makes subsequent tasks easier to solve is called representatioin learning. The learned representation is sometimes called the embedding space.
\end{frame}

\begin{frame}
    \frametitle{Transfer learning}
    \begin{itemize}
        \item The process of learning parameters using one task that are then applied to one or more other tasks is called pre-training:
        \begin{itemize}
            \item Send the new training data once through the fixed pre-trained network to obtain the training inputs in the new representation.
            \item Iterative gradient-based optimization can then be applied just to the smaller network consisting of the final layers.
        \end{itemize}
        \item Instead of using a pre-trained network as a fixed pre-processor, it is also possible to apply fine-tuning in which the whole network is adapted to the data for the new task.
        \item In multitask learning, a network jointly learns more than one related task at the same time. For example, spam email filter for different users.
        \item Meta-learning: Learn the learning algorithm itself. For example, few-shot learning, one-shot learning.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Contrastive learning}
    Contrastive learning focuses on extracting meaningful representations by contrasting positive and negative pairs of instances. Given:
    \begin{itemize}
        \item $x$: The anchor.
        \item $x^{+}$: Positive.
        \item $\{x^{-}_{1},\hdots,x^{-}_{N}\}$: Negative.
        \item $f_{w}$: The neural network function that maps points from input space to a representation space, governed by learnable parameters $w$. Futher, we require $||f_{w}(x)||=1$.
    \end{itemize}
    The loss function (called InfoNCE loss) is defined by:
    \begin{equation*}
        E(w)=-\log\frac{\exp(f_{w}(x)^{T}f_{w}(x^{+}))}{\exp(f_{w}(x)^{T}f_{w}(x^{+}))+\sum_{n=1}^{N}\exp(f_{w}(x)^{T}f_{w}(x^{-}_{n}))}
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{General network architectures}
    The neural network architecture does not have to be organized into a sequence of fully-connected layers. The network diagram only needs to be restricted to a feed-forward architecture. Each unit in such a network computes a function given by:
    \begin{equation*}
        z_{k}=h(\sum_{j\in\mathcal{A}(k)}w_{kj}z_{j}+b_{k})
    \end{equation*}
    where $\mathcal{A}(k)$ denotes the set of ancestors of node $k$, and $b_{k}$ denotes the associated bias parameter.
\end{frame}

\begin{frame}
    \frametitle{General network architectures}
    \begin{figure}
        \caption{A neural network having a general feed-forward topology}
        \includegraphics{Figure_15.pdf}
    \end{figure}
\end{frame}

\section{Error Functions}

\begin{frame}
    \frametitle{Regression}
    For single target variable:
    \begin{align*}
        p(t|x;w)&=\mathcal{N}(t;y(x;w),\sigma^{2}) \\
        E(w)&=\frac{1}{2}\sum_{n=1}^{N}(y(x_{n};w)-t_{n})^{2} \\
        \frac{\partial{}E}{\partial{}a_{n}}&=y(x_{n};w)-t_{n}
    \end{align*}
    For multiple target variables:
    \begin{align*}
        p(t|x;w)&=\mathcal{N}(t;y(x;w),\sigma^{2}I) \\
        E(w)&=\frac{1}{2}\sum_{n=1}^{N}||y(x^{n};w)-t^{n}||^{2} \\
        \frac{\partial{}E}{\partial{}a^{n}}&=(y(x^{n};w)-t^{n})^{T}
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Binary classification}
    For single target variable:
    \begin{align*}
        p(t|x;w)&=y(x;w)^{t}(1-y(x;w))^{1-t} \\
        E(w)&=-\sum_{n=1}^{N}(t_{n}\log{}y_{n}+(1-t_{n})\log(1-y_{n})) \\
        \frac{\partial{}E}{\partial{}a_{n}}&=y_{n}-t_{n}
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Binary classification}
    For multiple target variables:
    \begin{align*}
        p(t|x;w_{1},\hdots,w_{K})&=\prod_{k=1}^{K}y(x;w_{k})^{t_{k}}(1-y(x;w_{k}))^{1-t_{k}} \\
        E(w_{1},\hdots,w_{K})&=-\sum_{n=1}^{N}\sum_{k=1}^{K}(t^{n}_{k}\log{}y^{n}_{k}+(1-t^{n}_{k})\log(1-y^{n}_{k})) \\
        \frac{\partial{}E}{\partial{}a^{n}}&=(y^{n}-t^{n})^{T}
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Multiclass classification}
    \begin{align*}
        p(t|x;w_{1},\hdots,w_{K})&=\prod_{k=1}^{K}y_{k}(x;w_{1},\hdots,w_{K})^{t_{k}} \\
        E(w_{1},\hdots,w_{K})&=-\sum_{n=1}^{N}\sum_{k=1}^{K}t^{n}_{k}\log{}y^{n}_{k} \\
        \frac{\partial{}E}{\partial{}a^{n}}&=(y^{n}-t^{n})^{T}
    \end{align*}
\end{frame}

\section{Mixture Density Networks}

\begin{frame}
    \frametitle{Robot kinematics example}
    \begin{itemize}
        \item For many simple regression problems, the conditional distribution $p(t|x)$ we choose to model is Gaussian.
        \item Practical machine learning problems can often have significantly non-Gaussian distributions.
        \item Forward problem (many-to-one) vs. inverse problem (multimodal):
        \begin{itemize}
            \item Kinematics of a robot arm:
            \begin{itemize}
                \item Forward problem: Given the joint angles, find the end effector position.
                \item Inverse problem: Given the end effector position, find the joint angles.
            \end{itemize}
            \item Symptoms in the human body:
            \begin{itemize}
                \item Forward problem: Given the presence of a particular disease, find the pattern of symptoms.
                \item Inverse problem: Given the pattern of symptoms, predict the presence of a disease.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Robot kinematics example}
    \begin{figure}
        \caption{A robot kinematics example}
        \includegraphics{Figure_16_a.pdf}
        \includegraphics{Figure_16_b.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Conditional mixture distributions}
    The inverse problem for tutorial purposes:
    \begin{itemize}
        \item $\{x_{1},\hdots,x_{N}\}$ is uniformly sampled from $(0,1)$.
        \item $t_{n}=x_{n}+0.3\sin(2\pi{}x_{n})+\epsilon$, where $\epsilon$ is some uniform noise over the interval $(-0.1,0.1)$.
        \item The inverse problem is obtained by exchanging the roles of $x$ and $t$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conditional mixture distributions}
    \begin{figure}
        \caption{The inverse problem for tutorial purposes}
        \includegraphics{Figure_17_a.pdf}
        \includegraphics{Figure_17_b.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Conditional mixture distributions}
    We will model $p(t|x)$ using a mixture model in which both the mixing coefficients as well as the component densities are flexible functions of $x$:
    \begin{equation*}
        p(t|x)=\sum_{k=1}^{K}\pi_{k}(x)\mathcal{N}(t;\mu_{k}(x),\sigma^{2}_{k}(x)I)
    \end{equation*}
    where there are $K$ components in total, and $t\in\mathbb{R}^{L}$.
\end{frame}

\begin{frame}
    \frametitle{Conditional mixture distributions}
    We now take the various parameters of the mixture model to be governed by the outputs of a neural network that takes $x$ as its input:
    \begin{itemize}
        \item $\pi_{k}(x)$:
        \begin{itemize}
            \item $K$ output units, with pre-activations denoted by $a^{\pi}_{k}$.
            \item Activation function: $\pi_{k}(x)=\frac{\exp(a^{\pi}_{k})}{\sum_{j=1}^{K}\exp(a^{\pi}_{j})}$.
        \end{itemize}
        \item $\mu_{k}(x)$:
        \begin{itemize}
            \item $KL$ output units, with pre-activations denoted by $a^{\mu}_{kl}$.
            \item Activation function: The identity function, $\mu_{kl}(x)=a^{\mu}_{kl}$.
        \end{itemize}
        \item $\sigma_{k}(x)$:
        \begin{itemize}
            \item $K$ output units, with pre-activations denoted by $a^{\sigma}_{k}$.
            \item Activation function: $\sigma_{k}(x)=\exp(a^{\sigma}_{k})$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Gradient optimization}
    The error function (the negative log of the maximum likelihood) takes the form:
    \begin{equation*}
        E(w)=-\sum_{n=1}^{N}\log(\sum_{k=1}^{K}\pi_{k}(x^{n})\mathcal{N}(t^{n};\mu_{k}(x^{n}),\sigma^{2}_{k}(x^{n})I))
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Gradient optimization}
    Let's compute the derivatives of the error of the $n$th term with respect to the output-unit pre-activations:
    \begin{align*}
        \frac{\partial{}E_{n}}{\partial{}a^{\pi}_{k}}&=-\frac{1}{\sum_{j=1}^{K}\pi_{j}\mathcal{N}^{n}_{j}}\sum_{j=1}^{K}\mathcal{N}^{n}_{j}\frac{\partial\pi_{k}}{\partial{}a^{\pi}_{k}}=\pi_{k}-\frac{\pi_{k}\mathcal{N}^{n}_{k}}{\sum_{j=1}^{K}\pi_{j}\mathcal{N}^{n}_{j}} \\
        \frac{\partial{}E_{n}}{\partial{}a^{\mu}_{k}}&=-\frac{1}{\sum_{j=1}^{K}\pi_{j}\mathcal{N}^{n}_{j}}\pi_{k}\mathcal{N}^{n}_{k}(-\frac{1}{\sigma^{2}_{k}})(\mu_{k}-t^{n})^{T} \\
        &=\frac{\pi_{k}\mathcal{N}^{n}_{k}}{\sum_{j=1}^{K}\pi_{j}\mathcal{N}^{n}_{j}}\frac{(\mu_{k}-t^{n})^{T}}{\sigma^{2}_{k}} \\
        \frac{\partial{}E_{n}}{\partial{}a^{\sigma}_{k}}&=-\frac{1}{\sum_{j=1}^{K}\pi_{j}\mathcal{N}^{n}_{j}}\pi_{k}(-\frac{L}{\sigma_{k}}\mathcal{N}^{n}_{k}+\mathcal{N}^{n}_{k}\frac{||\mu_{k}-t^{n}||^{2}}{\sigma_{k}^{3}})\frac{\mathrm{d}\sigma_{k}}{\mathrm{d}a^{\sigma}_{k}} \\
        &=\frac{\pi_{k}\mathcal{N}^{n}_{k}}{\sum_{j=1}^{K}\pi_{j}\mathcal{N}^{n}_{j}}(L-\frac{||\mu_{k}-t^{n}||^{2}}{\sigma^{2}_{k}})
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Predictive distribution}
    Compute the conditional mean and variance, we see that:
    \begin{align*}
        E(t|x)&=\int{}tp(t|x)\mathrm{d}t=\sum_{k=1}^{K}\pi_{k}\int{t}\mathcal{N}(t;\mu_{k},\sigma^{2}_{k}I)\mathrm{d}t=\sum_{k=1}^{K}\pi_{k}\mu_{k} \\
        E(||t-x||^{2}|x)&=E(||t||^{2}|x)-||E(t|x)||^{2} \\
        &=\sum_{k=1}^{K}\pi_{k}(||\mu_{k}||^{2}+\sigma^{2}_{k})-||\sum_{k=1}^{K}\pi_{k}\mu_{k}||^{2}
    \end{align*}
    Compared with least-squares result, the mixture density network is more general:
    \begin{itemize}
        \item It can reproduce the least-squares result as a special case.
        \item The variance is more general because it is a function of $x$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Predictive distribution}
    \begin{figure}
        \caption{Plot of the approximate conditional mode}
        \includegraphics{Figure_19_d.pdf}
    \end{figure}
\end{frame}

\end{document}