\documentclass{beamer}

\usepackage{amsmath}

\usetheme{AnnArbor}
\usecolortheme{crane}
\usefonttheme[onlymath]{serif}

\title{Deep Learning - Foundations and Concepts}
\subtitle{Chapter 9. Regularization}
\author{nonlineark@github}
\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Inductive Bias}

\begin{frame}
    \frametitle{Inverse problems}
    \begin{itemize}
        \item Most machine learning tasks are examples of inverse problems:
        \begin{itemize}
            \item Forward problem: Given a conditional distribution $p(t|x)$ along with a finite set of input points $\{x_{1},\hdots,x_{N}\}$, sample corresponding values $\{t_{1},\hdots,t_{N}\}$ from that distribution.
            \item Inverse problem: Infer a distribution given only a finite number of samples.
        \end{itemize}
        \item We need a way to choose a specific distribution from amongst the infinitely many possibilities. The preference for one choice over others is called inductive bias or prior knowledge:
        \begin{itemize}
            \item Small changes in the input values should lead to small changes in the output values: The sum-of-squares regularizer.
            \item When detecting objects in images, there should be translation invariance: Convolutional neural network.
            \item Additional data from a different, but related, task can be used to help determine the learnable parameters in a neural network: Transfer learning and multi-task learning.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{No free lunch theorem}
    The no free lunch theorem: Every learning algorithm is as good as any other when averaged over all possible problems:
    \begin{itemize}
        \item Even very flexible neural networks contain important inductive biases, it is not possible to learn purely from data in the absence of any bias.
        \item In trying to find general-purpose learning algorithms, we are really seeking inductive biases that are appropriate to the broad classes of applications that will be encountered in practice.
        \item Inductive biases can be incorporated through:
        \begin{itemize}
            \item The form of distribution.
            \item The addition of a regularization term to the error function used during training.
            \item The training process.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Symmetry and invariance}
    \begin{itemize}
        \item In many applications of machine learning, the predictions should be unchanged under one or more transformations of the input variables:
        \begin{itemize}
            \item Translation invariance.
            \item Scale invariance.
        \end{itemize}
        \item Transformations that leave particular properties unchanged represent symmetries. The set of all transformations corresponding to a particular symmetry form a group.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Symmetry and invariance}
    Efficient approaches for encouraging an adaptive model to exhibit the required invarances:
    \begin{itemize}
        \item Pre-processing: Invariances are built into a pre-processing stage by computing features of the data that are invariant under the required transformations.
        \item Regularized error function: A regularization term is added to the error function to penalize changes in the model output when the input is subject to one of the invariant transformations.
        \item Data augmentation: The training set is expanded using replicas of the training data points, transformed according to the desired invariances and carrying the same output target values as the untransformed examples.
        \item Network architecture: The invariance properties are built into the structure of a neural network through an appropriate choice of network architecture.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Equivariance}
    A generalization of invariance is called equivariance in which the output of the network, instead of remaining constant when the input is transformed, is itself transformed in a specific way:
    \begin{equation*}
        \mathcal{S}(\mathcal{T}(I))=\tilde{\mathcal{T}}(\mathcal{S}(I))
    \end{equation*}
    For example:
    \begin{itemize}
        \item $\mathcal{S}$: Measures the orientation of an object within an image.
        \item $\mathcal{T}$: Rotation.
        \item $\tilde{\mathcal{T}}$: Increase or decrease the scalar orientation.
    \end{itemize}
\end{frame}

\section{Weight Decay}

\begin{frame}
    \frametitle{Weight decay}
    \begin{itemize}
        \item The simplest regularizer comprises the sum of the squares of the model parameters: $\tilde{E}(w)=E(w)+\frac{\lambda}{2}w^{T}w$.
        \item The sum-of-squares regularizer is known as weight decay because it encourages weight values to decay towards zero.
        \item The sum-of-squares regularizer can be interpreted as the negative logarithm of a zero-mean Gaussian prior distribution over the weight vector $w$, which is a way to include prior knowledge into the model training process.
        \item It is trivial to evaluate its derivatives for use in gradient descent training: $\nabla\tilde{E}(w)=\nabla{}E(w)+\lambda{}w$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Weight decay}
    Consider a two-dimensional parameter space along with a sum-of-squares error function $E(w)$:
    \begin{itemize}
        \item The effect of the regularization term is to shrink the magnitudes of the weight parameters.
        \item The effect is much larger for the parameter corresponds to the smaller eigenvalue (here $w_{1}$) compared to that of the parameter corresponds to the larger eigenvalue (here $w_{2}$).
        \item The effective number of parameters is the number that remain active after regularization.
        \item Controlling model complexity by regularization has similarities to controlling model complexity by limiting the number of parameters:
        \begin{itemize}
            \item $\lambda\to\infty$: The effective number of parameters is zero.
            \item $\lambda=0$: The effective number of parameters equals the total number of parameters in the model.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Weight decay}
    \begin{figure}
        \caption{Effect of a quadratic regularizer}
        \includegraphics[height=0.7\textheight]{Figure_3.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Consistent regularizers}
    Consider a two-layer network of the form:
    \begin{align*}
        z_{j}&=h(\sum_{i}w_{ji}x_{i}+w_{j0})&y_{k}&=\sum_{j}w_{kj}z_{j}+w_{k0}
    \end{align*}
    Observe that linear transformations of the input data or output variables can be balanced by making a corresponding linear transformation of the weights and biases so that the mapping performed by the network is unchanged:
    \begin{align*}
        x_{i}&\to\tilde{x}_{i}=ax_{i}+b&y_{k}&\to\tilde{y}_{k}=cy_{k}+d \\
        w_{ji}&\to\tilde{w}_{ji}=\frac{1}{a}w_{ji}&w_{kj}&\to\tilde{w}_{kj}=cw_{kj} \\
        w_{j0}&\to\tilde{w}_{j0}=w_{j0}-\frac{b}{a}\sum_{i}w_{ji}&w_{k0}&\to\tilde{w}_{k0}=cw_{k0}+d
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Consistent regularizers}
    Clearly, the simple sum-of-squares weight decay does not have this consistency. We look for a regularizer that is invariant to re-scaling of the weights and to shifts of the biases:
    \begin{equation*}
        \frac{\lambda_{1}}{2}\sum_{w\in\mathcal{W}_{1}}w^{2}+\frac{\lambda_{2}}{2}\sum_{w\in\mathcal{W}_{2}}w^{2}
    \end{equation*}
    where $\mathcal{W}_{1}$ denotes the set of weights in the first layer, $\mathcal{W}_{2}$ denotes the set of weights in the second layer. Because biases are excluded from the summations, the prior distribution over the parameters is improper:
    \begin{equation*}
        p(w;\lambda_{1},\lambda_{2})\propto\exp(-\frac{\lambda_{1}}{2}\sum_{w\in\mathcal{W}_{1}}w^{2}-\frac{\lambda_{2}}{2}\sum_{w\in\mathcal{W}_{2}}w^{2})
    \end{equation*}
    It is therefore common to include separate priors for the biases (which then break the shift invariance) that have their own hyperparameters.
\end{frame}

\begin{frame}
    \frametitle{Consistent regularizers}
    More generally, we can consider regularizers in which the weights are divided into any number of groups $\mathcal{W}_{k}$ so that:
    \begin{equation*}
        \Omega(w)=\frac{1}{2}\sum_{k}\lambda_{k}||w||_{k}^{2}=\frac{1}{2}\sum_{k}\lambda_{k}\sum_{w\in\mathcal{W}_{k}}w^{2}
    \end{equation*}
    For example, we could use a different regularizer for each layer in a multilayer network.
\end{frame}

\begin{frame}
    \frametitle{Generalized weight decay}
    Generalization of the simple quadratic regularizer:
    \begin{equation*}
        \Omega(w)=\frac{\lambda}{2}\sum_{m=1}^{M}|w_{m}|^{q}
    \end{equation*}
    where:
    \begin{itemize}
        \item $q=2$ corresponds to the quadratic regularizer.
        \item $q=1$ is known as a lasso. For quadratic error functions, it has the property that if $\lambda$ is sufficiently large, some of the coefficients $w_{j}$ are driven to zero, leading to a sparse model.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Generalized weight decay}
    \begin{figure}
        \caption{Lasso vs. quadratic regularizer}
        \includegraphics[width=0.4\textwidth]{Figure_6_a.pdf}
        \includegraphics[width=0.4\textwidth]{Figure_6_b.pdf}
    \end{figure}
\end{frame}

\section{Learning Curves}

\begin{frame}
    \frametitle{Learning curves}
    During optimization of the error function through gradient descent:
    \begin{itemize}
        \item The training error typically decreases as the model parameters are updated.
        \item The error for validation data may be non-monotonic.
    \end{itemize}
    This behavior can be visualized using learning curves, which provide insight into the progress of training and also offer a practical methodology for controlling the effective model complexity.
\end{frame}

\begin{frame}
    \frametitle{Early stopping}
    To obtain a network with good generalization performance, training should be stopped at the point of smallest error with respect to the validation data set.
    \begin{figure}
        \caption{An illustration of the behavior of training set error and validation set error during a typical training session}
        \includegraphics[width=0.4\textwidth]{Figure_7_a.pdf}
        \includegraphics[width=0.4\textwidth]{Figure_7_b.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Early stopping}
    This behavior of the learning curves is sometimes explained qualitatively in terms of the effective number of parameters in the network:
    \begin{itemize}
        \item This number starts out small and grows during training.
        \item Early stopping is a way to limit the effective network complexity.
    \end{itemize}
    \begin{figure}
        \caption{Early stopping can give similar results to weight decay for a quadratic error function}
        \includegraphics[height=0.4\textheight]{Figure_8.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Double descent}
    Modern deep neural networks defeat the conventional belief:
    \begin{itemize}
        \item Conventional belief: The number of parameters in the model needs to be limited according to the size of the data set and that for a given training data set, very large models are expected to have poor performance.
        \item The general wisdom in the deep learning community is that bigger models are better.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Double descent}
    \begin{figure}
        \caption{Plot of training set and test set errors for a large neural network model versus the complexity of the model}
        \includegraphics[height=0.6\textheight]{Figure_9.jpg}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Double descent}
    The surprising behavior exhibits two different regimes of model fitting:
    \begin{itemize}
        \item The classical bias-variance trade-off for small to medium complexity.
        \item Followed by a further reduction in test set error as we enter a regime of very large models.
        \item The transition between the two regimes occurs roughly when the model is complex enough that the model is able to fit the training data exactly:
        \begin{itemize}
            \item Effective model complexity: The maximum size of training data set on which a model can achieve zero training error.
            \item Double descent arises when the effective model complexity exceeds the number of data points in the training set.
            \item It is possible to operate in a regime where increasing the size of the training data set could actually reduce performance.
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Parameter Sharing}

\begin{frame}
    \frametitle{Parameter sharing}
    Weight sharing (parameter sharing or parameter tying):
    \begin{itemize}
        \item Impose hard constraints on the weights by forming them into groups and requiring that all weights within each group share the same value.
        \item Because the number of degrees of freedom is smaller than the number of connections in the network, weight sharing reduces network complexity.
        \item Parameter sharing is extensively used in convolutional neural networks.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Soft weight sharing}
    We can encourage the weight values to form several groups by considering a probability distribution that is a mixture of Gaussians:
    \begin{equation*}
        p(w)=\prod_{i}\sum_{k=1}^{K}\pi_{k}\mathcal{N}(w_{i};\mu_{k},\sigma^{2}_{k})
    \end{equation*}
    where $K$ is the number of components in the mixture. Taking the negative logarithm then leads to a regularization function of the form:
    \begin{equation*}
        \Omega(w)=-\sum_{i}\log\sum_{k=1}^{K}\pi_{k}\mathcal{N}(w_{i};\mu_{k},\sigma^{2}_{k})
    \end{equation*}
    The total error function is then given by:
    \begin{equation*}
        \tilde{E}(w)=E(w)+\lambda\Omega(w)
    \end{equation*}
    where $\lambda$ is the regularization coefficient.
\end{frame}

\begin{frame}
    \frametitle{Soft weight sharing}
    The error $\tilde{E}(w)$ is minimized jointly with respect to the weights $\{w_{i}\}$ and with respect to the parameters $\{\pi_{k},\mu_{k},\sigma_{k}\}$ of the mixture model, which requires that we evaluate the derivatives of $\Omega(w)$ with respect to all the learnable parameters.
    \bigbreak
    First, let's introduce:
    \begin{equation*}
        \gamma_{k}(w_{i})=\frac{\pi_{k}\mathcal{N}(w_{i};\mu_{k},\sigma^{2}_{k})}{\sum_{j}\pi_{j}\mathcal{N}(w_{i};\mu_{j},\sigma^{2}_{j})}
    \end{equation*}
    We could understand $\gamma_{k}(w_{i})$ this way:
    \begin{itemize}
        \item $\pi_{k}$ is the prior for the $k$th component $\mathcal{C}_{k}$: $\pi_{k}=p(\mathcal{C}_{k})$.
        \item $\mathcal{N}(w_{i};\mu_{k},\sigma^{2}_{k})=p(w_{i}|\mathcal{C}_{k})$.
        \item Then $\gamma_{k}(w_{i})$ is the posterior for $\mathcal{C}_{k}$: $\gamma_{k}(w_{i})=p(\mathcal{C}_{k}|w_{i})$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Soft weight sharing}
    Now let's calculate the derivatives:
    \begin{align*}
        \frac{\partial\tilde{E}}{\partial{}w_{i}}&=\frac{\partial{}E}{\partial{}w_{i}}+\lambda\sum_{k}\gamma_{k}(w_{i})\frac{w_{i}-\mu_{k}}{\sigma^{2}_{k}} \\
        \pi_{k}&=\frac{\exp(\eta_{k})}{\sum_{j}\exp(\eta_{j})}\qquad\frac{\partial\tilde{E}}{\partial\eta_{k}}=\lambda\sum_{i}(\pi_{k}-\gamma_{k}(w_{i})) \\
        \frac{\partial\tilde{E}}{\partial\mu_{k}}&=\lambda\sum_{i}\gamma_{k}(w_{i})\frac{\mu_{k}-w_{i}}{\sigma^{2}_{k}} \\
        \sigma^{2}_{k}&=\exp(\xi_{k})\qquad\frac{\partial\tilde{E}}{\partial\xi_{k}}=\frac{\lambda}{2}\sum_{i}\gamma_{k}(w_{i})(1-\frac{(w_{i}-\mu_{k})^{2}}{\sigma^{2}_{k}})
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Soft weight sharing}
    When minimizing $\tilde{E}(w)$, we are driving all the derivatives towards zero. We see that each $w_{i}$ is pulled towards the center of the $k$th Gaussian, with a force proportional to the posterior probability of that Gaussian for $w_{i}$. And for $\{\pi_{k},\mu_{k},\sigma_{k}\}$:
    \begin{align*}
        \pi_{k}&\to\frac{1}{W}\sum_{i}\gamma_{k}(w_{i}) \\
        \mu_{k}&\to\frac{\sum_{i}\gamma_{k}(w_{i})w_{i}}{\sum_{i}\gamma_{k}(w_{i})} \\
        \sigma^{2}_{k}&\to\frac{\sum_{i}\gamma_{k}(w_{i})(w_{i}-\mu_{k})^{2}}{\sum_{i}\gamma_{k}(w_{i})}
    \end{align*}
\end{frame}

\section{Residual Connections}

\begin{frame}
    \frametitle{Shattered gradients}
    \begin{itemize}
        \item The representational capabilities of neural networks increase exponentially with depth. With $\mathrm{ReLU}$ activation functions, there is an exponential increase in the number of linear regions that the network can represent.
        \item However, a consequence of this is a proliferation of discontinuities in the gradient of the error function.
    \end{itemize}
    \begin{figure}
        \caption{Jacobian for networks with a single input and a single output}
        \includegraphics[width=0.8\textwidth]{Figure_12.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Residual connections}
    An important modification to the architecture of neural networks that greatly assists in training very deep networks is that of residual connections, which consist simply of adding the input to each function back onto the output:
    \begin{align*}
        z_{1}&=F_{1}(x)&z_{1}&=F_{1}(x)+x \\
        z_{2}&=F_{2}(z_{1})&z_{2}&=F_{2}(z_{1})+z_{1} \\
        y&=F_{3}(z_{2})&y&=F_{3}(z_{2})+z_{2}
    \end{align*}
    The gradients in a network with residual connections are much less sensitive to input values compared to a standard deep network. The effect of the residual connections is to create smoother error function surfaces, because the error surfaces are moderated by a combination of shallow and deep sub-networks.
\end{frame}

\begin{frame}
    \frametitle{Residual connections}
    \begin{figure}
        \caption{A residual network and its expanded form}
        \includegraphics[width=0.8\textwidth]{Figure_13.pdf}
        \includegraphics[width=0.8\textwidth]{Figure_15.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Residual connections}
    \begin{figure}
        \caption{Two alternative ways to include residual connections into a standard feed-forward network}
        \includegraphics[width=0.8\textwidth]{Figure_16_a.pdf}
        \includegraphics[width=0.8\textwidth]{Figure_16_b.pdf}
    \end{figure}
\end{frame}

\section{Model Averaging}

\begin{frame}
    \frametitle{Model averaging}
    If we have several different models trained to solve the same problem, we can often improve generalization by averaging the predictions made by the individual models. Consider a regression problem, suppose:
    \begin{itemize}
        \item We have a set of trained models: $y_{1}(x),\hdots,y_{M}(x)$.
        \item The true function that we are trying to predict is given by $h(x)$.
        \item The output of each of the models can be written as the true value plus an error: $y_{m}(x)=h(x)+\epsilon_{m}(x)$.
        \item We form a committee model given by: $y_{\textrm{COM}}(x)=\frac{1}{M}\sum_{m=1}^{M}y_{m}(x)$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Model averaging}
    Further, let's suppose that:
    \begin{itemize}
        \item The errors have zero mean: $E(\epsilon_m(x))=0$.
        \item Errors from different models are uncorrelated.
    \end{itemize}
    If we calculate the expected error from the committee, we see that:
    \begin{align*}
        E_{\textrm{COM}}&=E((y_{\textrm{COM}}(x)-h(x))^{2})=E((\frac{1}{M}\sum_{m=1}^{M}y_{m}(x)-h(x))^{2}) \\
        &=E((\frac{1}{M}\sum_{m=1}^{M}\epsilon_{m}(x))^{2})=\frac{1}{M^{2}}\sum_{m=1}^{M}E(\epsilon^{2}_m(x)) \\
        &=\frac{1}{M}(\frac{1}{M}\sum_{m=1}^{M}E((y_{m}(x)-h(x))^{2}))=\frac{1}{M}E_{\textrm{AV}}
    \end{align*}
    where $E_{\textrm{AV}}$ is the average error made by the models acting individually.
\end{frame}

\begin{frame}
    \frametitle{Dropout}
    \begin{itemize}
        \item The central idea of dropout is to delete nodes from the network, including their connections, at romdom during training.
        \item Dropout is applied to both hidden nodes and input nodes, but not outputs.
        \item It can be implemented by defining a mask vector $R_{i}\in\{0,1\}$ which multiplies the activation of the non-output node $i$, whose values are set to $1$ with probability $\rho$.
        \item $\rho=0.5$ seems to work well for the hidden nodes, whereas for the inputs a value of $\rho=0.8$ is typically used.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Dropout}
    Once training is complete, predictions can in principle be made by:
    \begin{equation*}
        p(y|x)=\sum_{R}p(R)p(y|x,R)
    \end{equation*}
    where the sum is over the exponentially large space of masks, and $p(y|x,R)$ is the predictive distribution from the network with mask $R$.
    \begin{itemize}
        \item This summation is intractable, and in practice, as few as $10$ or $20$ masks can be sufficient to obtain good results. This procedure is known as Monte Carlo dropout.
        \item A simpler approach is to make predictions using the trained network with no nodes masked out. The weights in the network need to be re-scaled to compensate for the fact that in training a proportion of the nodes would be missing.
    \end{itemize}
\end{frame}

\end{document}