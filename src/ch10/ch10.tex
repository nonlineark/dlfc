\documentclass{beamer}

\usepackage{amsmath}

\usetheme{AnnArbor}
\usecolortheme{crane}
\usefonttheme[onlymath]{serif}

\title{Deep Learning - Foundations and Concepts}
\subtitle{Chapter 10. Convolutional Networks}
\author{nonlineark@github}
\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Computer Vision}

\begin{frame}
    \frametitle{Computer vision}
    \begin{itemize}
        \item Computer vision was one of the first fields to be transformed by the deep learning revolution, predominantly thanks to the CNN architecture.
        \item Recently alternative architectures based on transformers have become competitive with convolutional networks in some applications.
        \item Some applications for machine learning in computer vision: Classification, detection, segmentation, caption generation, synthesis, inpainting, style transfer, super-resolution, depth prediction, scene reconstruction.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Image data}
    \begin{itemize}
        \item The structure of an image:
        \begin{itemize}
            \item An image comprises a rectangular array of pixels.
            \item Each pixel has either a grey-scale intensity or a triplet of red, green and blue channels each with its own intensity value.
        \end{itemize}
        \item Challenges of applying neural networks to image data:
        \begin{itemize}
            \item Images generally have a high dimensionality.
            \item Image data is highly structured.
        \end{itemize}
        \item Local correlations can be used to encode strong inductive biases into a neural network, leading to models with far fewer parameters and with much better generalization accuracy.
    \end{itemize}
\end{frame}

\section{Convolutional Filters}

\begin{frame}
    \frametitle{Inductive biases}
    To exploit the two-dimensional structure of image data to create inductive biases, we can use four interrelated concepts:
    \begin{itemize}
        \item Hierarchy.
        \item Locality.
        \item Equivariance.
        \item Invariance.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Feature detectors}
    Let's restrict our attention to grey-scale images first. Consider a single unit in the first layer of a neural network:
    \begin{itemize}
        \item It takes as input just the pixel values from a small rectangular region, or patch, from the image. This patch is referred to as the receptive field of that unit.
        \item The output of this unit is given by the usual functional form comprising a weighted linear combinations of the input values, which is subsequently transformed using a nonlinear activation function: $z=\mathrm{ReLU}(w^{T}x+w_{0})$.
        \item The weights themselves form a small two-dimensional grid known as a filter, sometimes also called a kernel.
        \item The unit acts as a feature detector that signals when it finds a sufficiently good match to its kernel (basic properties of inner product).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Feature detectors}
    \begin{figure}
        \caption{A single unit in the first layer of a neural network}
        \includegraphics[height=0.7\textheight]{Figure_1_a.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Translation equivariance}
    \begin{itemize}
        \item To generalize what our neural network has learned in one location to all possible locations in the image, we can simply replicate the same hidden-unit weight values at multiple locations across the image.
        \item The units of the hidden layer form a feature map in which all the units share the same weights.
    \end{itemize}
    For an image $I$ with pixel intensities $I(j,k)$, and a filter $K$ with pixel values $K(l,m)$, the feature map $C$ has activation values given by:
    \begin{equation*}
        C(j,k)=\sum_{l}\sum_{m}I(j+l,k+m)K(l,m)
    \end{equation*}
    where we have omitted the nonlinear activation function for clarity. This is an example of a convolution and is sometimes expressed as $C=I*K$.
\end{frame}

\begin{frame}
    \frametitle{Translation equivariance}
    \begin{figure}
        \caption{A $3\times{}3$ image convolved with a $2\times{}2$ filter}
        \includegraphics{Figure_3.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Translation equivariance}
    Comparing this convolutional structure with a standard fully connected network, we see several advantages:
    \begin{itemize}
        \item The connections are sparse, leading to far fewer weights even with large images.
        \item The weight values are shared, greatly reducing the number of independent parameters and consequently reducing the required size of the training set.
        \item The same network can be applied to images of different sizes without the need for retraining.
        \item Convolutions can be implemented very efficiently by exploiting the massive parallelism of GPUs.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Padding}
    Consider an image of dimensionality $J\times{}K$ and a kernal of dimensionality $M\times{}M$:
    \begin{itemize}
        \item The feature map is of dimensionality $(J-M+1)\times(K-M+1)$, which is smaller than the original image.
        \item We can pad the original image with additional pixels around the outside. If we pad with $P$ pixels then the output map has dimensionality $(J+2P-M+1)\times(K+2P-M+1)$:
        \begin{itemize}
            \item If $P=0$, this is called a valid convolution.
            \item If $P=\frac{M-1}{2}$, this is called a same convolution.
        \end{itemize}
        \item Padding values are typically set to zero, after first subtracting the mean from each image so that zero represents the average value of the pixel intensity.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Strided convolutions}
    Sometimes we wish to use feature maps that are significantly smaller than the original image to provide flexibility in the design of convolutional network architectures. One way to achieve this is to use strided convolutions in which, the filter is moved in larger steps of size $S$, called the stride. The feature map will have dimensionality:
    \begin{equation*}
        (\lfloor\frac{J+2P-M}{S}\rfloor+1)\times(\lfloor\frac{K+2P-M}{S}\rfloor+1)
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Multi-dimensional convolutions}
    We can easily extend convolutions to cover multiple channels by extending the dimensionality of the filter:
    \begin{itemize}
        \item An image with $J\times{}K$ pixels and $C$ channels will be described by a tensor of dimensionality $J\times{}K\times{}C$.
        \item We can introduce a filter described by a tensor of dimensionality $M\times{}M\times{}C$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Multi-dimensional convolutions}
    \begin{figure}
        \caption{A multi-dimensional filter}
        \includegraphics[height=0.7\textheight]{Figure_6_a.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Multi-dimensional convolutions}
    To build more flexible models, we simply include multiple filters, in which each filter has its own independent set of parameters giving rise to its own independent feature map:
    \begin{itemize}
        \item We again refer to these separate feature maps as channels.
        \item The filter tensor now has dimensionality $M\times{}M\times{}C\times{}C_{\textrm{OUT}}$ where $C$ is the number of input channels and $C_{\textrm{OUT}}$ is the number of output channels.
        \item Each output channel will have its own associated bias parameter, so the total number of parameters will be $(M^{2}C+1)C_{\textrm{OUT}}$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Pooling}
    Small changes in the relative locations do not affect the classification, and we want to be invariant to such small translations of individual features. This can be achieved using pooling:
    \begin{itemize}
        \item A pooling layer is similar to a convolutional layer:
        \begin{itemize}
            \item Each unit takes input from a receptive field in the previous feature map layer.
            \item There is a choice of filter size and of stride length.
        \end{itemize}
        \item The difference is that the output of a pooling unit is a simple, fixed function of its inputs:
        \begin{itemize}
            \item Max-pooling.
            \item Average pooling.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Pooling}
    \begin{figure}
        \caption{Illustration of max-pooling}
        \includegraphics{Figure_8.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Multilayer convolutions}
    \begin{itemize}
        \item To allow the network to discover and represent hierarchical structure in the data, we extend the architecture by considering multiple layers.
        \item Each convolutional layer is described by a filter tensor of dimensionality $M\times{}M\times{}C_{\textrm{IN}}\times{}C_{\textrm{OUT}}$ in which the number of independent weight and bias parameters is $(M^{2}C_{\textrm{IN}}+1)C_{\textrm{OUT}}$.
        \item Each such convolutional layer can optionally be followed by a pooling layer.
        \item In many applications (e.g., classification tasks), the output units need to combine information from across the whole of the input image. This is typically achieved by introducing one or two standard fully connected layers as the final stages of the network.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Example network architectures}
    \begin{figure}
        \caption{The architecture of a convolutional network called VGG-16}
        \includegraphics[width=0.8\textwidth]{Figure_10.pdf}
    \end{figure}
\end{frame}

\section{Visualizing Trained CNNs}

\begin{frame}
    \frametitle{Visual cortex}
    Historically, much of the motivation for CNNs came from pioneering research in neuroscience:
    \begin{itemize}
        \item Simple cells: Respond to visual inputs with a simple edge oriented at a particular angle and located at a particular position within the visual field.
        \item Complex cells: Respond to more complex stimuli and which seem to be derived by combining and processing the output of simple cells.
        \item Grandmother cells: Respond if, and only if, the visual input corresponds to a person's grandmother, irrespective of location, scale, lighting, or other transformations of the scene.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Visual cortex}
    The response of the simple cells can be modelled using Gabor filters:
    \begin{align*}
        G(x,y)&=A\exp(-\alpha\tilde{x}^{2}-\beta\tilde{y}^{2})\sin(\omega\tilde{x}+\phi) \\
        \tilde{x}&=(x-x_{0})\cos\theta+(y-y_{0})\sin\theta \\
        \tilde{y}&=-(x-x_{0})\sin\theta+(y-y_{0})\cos\theta
    \end{align*}
    \begin{itemize}
        \item The $\sin$ term represents a sinusoidal spatial oscillation oriented in a direction defined by the polar angle $\theta$, with frequency $\omega$ and phase angle $\phi$.
        \item The exponential factor creates a decay envelope that localizes the filter in the neighborhood of position $(x_{0},y_{0})$ and with decay rates governed by $\alpha$ and $\beta$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Visual cortex}
    \begin{figure}
        \caption{Examples of Gabor filters}
        \includegraphics[height=0.7\textheight]{Figure_11.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Visualizing trained filters}
    For the filters in the first convolutional layer, we can visualize the network weights associated with these filters directly as small images (Cauchy-Schwarz inequality).
    \bigbreak
    We see a remarkable similarity between these filters and the Gabor filters. This is because these characteristic filters are a general property of the statistics of natural images and therefore prove useful for image understanding in both natural and artificial systems.
\end{frame}

\begin{frame}
    \frametitle{Visualizing trained filters}
    \begin{figure}
        \caption{Examples of learned filters from the first layer of AlexNet}
        \includegraphics[height=0.7\textheight]{Figure_12.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Visualizing trained filters}
    One approach to visualize the filters in subsequent layers is to present a large number of image patches to the network and see which produce the highest activation value in any particular hidden unit.
    \begin{figure}
        \caption{Examples of image patches that produce the strongest activation in the hidden units in a network}
        \includegraphics[width=0.8\textwidth]{Figure_13.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Visualizing trained filters}
    We can extend this technique to perform a numerical optimization over the input variables to maximize the activation of a particular unit. If we chose the unit to be one of the outputs then we can look for an image that is most representative of the corresponding class label.
    \begin{figure}
        \caption{Examples of synthetic images generated by maximizing the class probability}
        \includegraphics[width=0.8\textwidth]{Figure_14.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Saliency maps}
    \begin{itemize}
        \item Saliency maps aim to identify those regions of an image that are most significant in determining the class label.
        \item This is best done by investigating the final convolutional layer:
        \begin{itemize}
            \item This layer still retains spatial localization.
            \item This layer has the highest level of semantic representation.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Saliency maps}
    The Grad-CAM (gradient class activation mapping) method. For a given input image:
    \begin{itemize}
        \item First compute the derivatives of the output-unit pre-activation $a^{c}$ for a given class $c$ with respect to the pre-activations $a_{ij}^{(k)}$ of all the units in the final convolutional layer for channel $k$.
        \item For each channel in that layer, the average of those derivatives is evaluated to give: $\alpha_{k}=\frac{1}{M_{k}}\sum_{i}\sum_{j}\frac{\partial{}a^{c}}{\partial{}a_{ij}^{(k)}}$, where $M_{k}$ is the total number of units in that channel.
        \item These averages are then used to form a weighted sum of the form: $L=\sum_{k}\alpha_{k}A^{(k)}$, where $A^{(k)}$ is a matrix with elements $a_{ij}^{(k)}$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Saliency maps}
    \begin{figure}
        \caption{Saliency maps for the VGG-16 network with respect to the dog and cat categories}
        \includegraphics{Figure_15.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Adversarial attacks}
    \begin{itemize}
        \item Adversarial attacks involve making very small modifications to an image, at level that is imperceptible to a human, which cause the image to be misclassified by the neural network.
        \item The fast gradient sign method: Changing each pixel value in an image $x$ by a fixed amount $\epsilon$ with a sign determined by the gradient of an error function $E(x,t)$ with respect to the pixel values: $x'=x+\epsilon\mathrm{sign}(\nabla_{x}E(x,t))$, where $t$ is the true label of $x$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Synthetic images}
    DeepDream: Determine which nodes in a particular hidden layer of the network respond strongly to a particular image and then modify the image to amplify those responses:
    \begin{itemize}
        \item We apply an image to the input of the network and forward propagate through to some particular layer.
        \item We then set the backpropagation $\delta$ variables for that layer equal to the pre-activations of the nodes and run backpropagation to the input layer to get a gradient vector over the pixels of the image.
        \item Finally, we modify the image by taking a small step in the direction of the gradient vector.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Synthetic images}
    \begin{figure}
        \caption{Examples of DeepDream applied to an image}
        \includegraphics[height=0.6\textheight]{Figure_18.pdf}
    \end{figure}
\end{frame}

\end{document}