\documentclass{beamer}

\usepackage[ruled]{algorithm2e}
\SetKw{KwRet}{return}
\usepackage{amsmath}

\usetheme{AnnArbor}
\usecolortheme{crane}
\usefonttheme[onlymath]{serif}

\title{Deep Learning - Foundations and Concepts}
\subtitle{Chapter 14. Sampling}
\author{nonlineark@github}
\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Basic Sampling Algorithms}

\begin{frame}
    \frametitle{Expectations}
    For some applications the goal is to evaluate expectations with respect to the distribution. Suppose we wish to find the expectation of a function $f(z)$ with respect to a probability distribution $p(z)$:
    \begin{equation*}
        E(f)=\int{}f(z)p(z)\mathrm{d}z
    \end{equation*}
    The general idea behind sampling methods is to obtain a set of samples $z^{(l)}$ drawn independently from the distribution $p(z)$. This allows the expectation to be approximated by a finite sum:
    \begin{equation*}
        \bar{f}=\frac{1}{L}\sum_{l=1}^{L}f(z^{(l)})
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Expectations}
    Let's calculate the expectation and variance of $\bar{f}$:
    \begin{align*}
        E(\bar{f})&=E(\frac{1}{L}\sum_{l=1}^{L}f(z^{(l)}))=E(f) \\
        E(\bar{f}^{2})&=E(\frac{1}{L^{2}}\sum_{l,l'}f(z^{(l)})f(z^{(l')}))=(E(f))^{2}+\frac{1}{L}\mathrm{var}(f) \\
        \mathrm{var}(\bar{f})&=E(\bar{f}^{2})-(E(\bar{f}))^{2}=\frac{1}{L}\mathrm{var}(f)
    \end{align*}
    Which shows that:
    \begin{itemize}
        \item $\bar{f}$ is an unbiased estimator of $E(f)$.
        \item Due to the linear decrease of the variance with increasing $L$, in principle, high accuracy may be achievable with a relatively small number of samples $z^{(l)}$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Standard distributions}
    \begin{block}{Problem}
        Suppose that $z$ is uniformly distributed over the interval $(0,1)$. Given a probability density function $p$, find a function $g$ such that the random variable $y=g(z)$ has $p$ as its probability density function.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Standard distributions}
    Let $U$ be the probability density function of the uniform distribution over the interval $(0,1)$, we have:
    \begin{align*}
        p(y)\mathrm{d}y&=U(z)\mathrm{d}z \\
        f(y_{0})&=\int_{-\infty}^{y_{0}}p(y)\mathrm{d}y=\int_{-\infty}^{z_{0}}U(z)\mathrm{d}z=z_{0} \\
        y_{0}&=f^{-1}(z_{0})
    \end{align*}
    So we have to transform the uniformly distributed random numbers using a function that is the inverse of the cumulative distribution function of the desired probability density function.
\end{frame}

\begin{frame}
    \frametitle{Standard distributions}
    Some examples:
    \begin{itemize}
        \item Exponential distribution $p(y)=\lambda\exp(-\lambda{}y)$:
        \begin{itemize}
            \item $z=f(y)=\int_{0}^{y}p(t)\mathrm{d}t=1-\exp(-\lambda{}y)$.
            \item $y=-\frac{1}{\lambda}\log(1-z)$.
        \end{itemize}
        \item Cauchy distribution $p(y)=\frac{1}{\pi}\frac{1}{1+y^{2}}$:
        \begin{itemize}
            \item $z=f(y)=\int_{-\infty}^{y}p(t)\mathrm{d}t=\frac{1}{\pi}\arctan{}y+\frac{1}{2}$.
            \item $y=\tan(\pi(z-\frac{1}{2}))$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Standard distributions}
    The generalization to multiple variables involves the Jacobian of the change of variables, so that:
    \begin{equation*}
        p_{Y}(y_{1},\hdots,y_{M})=p_{Z}(z_{1},\hdots,z_{M})|\frac{\partial(z_{1},\hdots,z_{M})}{\partial(y_{1},\hdots,y_{M})}|
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Standard distributions}
    The Box-Muller method for generating samples from a Gaussian distribution. First, suppose we generate pairs of uniformly distributed random numbers $z_{1},z_{2}\in(-1,1)$. Next, we discard each pair unless it satisfies $z_{1}^{2}+z_{2}^{2}\le{}1$. This leads to a uniform distribution of points inside the unit circle with $p_{Z}(z_{1},z_{2})=\frac{1}{\pi}$. Then, for each pair $z_{1},z_{2}$ we evaluate the quantities:
    \begin{equation*}
        y=z\frac{\sqrt{-4\log||z||}}{||z||}
    \end{equation*}
    The joint distribution of $y_{1}$ and $y_{2}$ is given by:
    \begin{equation*}
        p_{Y}(y_{1},y_{2})=p_{Z}(z_{1},z_{2})|\frac{\partial(z_{1},z_{2})}{\partial(y_{1},y_{2})}|=(\frac{1}{\sqrt{2\pi}}\exp(-\frac{y_{1}^{2}}{2}))(\frac{1}{\sqrt{2\pi}}\exp(-\frac{y_{2}^{2}}{2}))
    \end{equation*}
    So $y_{1}$ and $y_{2}$ are independent and each has a Gaussian distribution with zero mean and unit variance.
\end{frame}

\begin{frame}
    \frametitle{Rejection sampling}
    Suppose that:
    \begin{itemize}
        \item We wish to sample from a distribution $p(z)$, and sampling directly from $p(z)$ is difficult.
        \item We are easily able to evaluate $p(z)$ for any given value of $z$, up to some normalizing constant $Z_{p}$, so that $p(z)=\frac{1}{Z_{p}}\tilde{p}(z)$, where $\tilde{p}(z)$ can readily be evaluated, but $Z_{p}$ is unknown.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Rejection sampling}
    To apply rejection sampling:
    \begin{itemize}
        \item Find a simpler distribution $q(z)$, called a proposal distribution, from which we can readily draw samples.
        \item Introduce a constant $k$ whose value is chosen such that $kq(z)\ge\tilde{p}(z)$ for all values of $z$.
        \item Generate a number $z_{0}$ from the distribution $q(z)$.
        \item Generate a number $u_{0}$ from the uniform distribution over $[0,kq(z_{0})]$.
        \item If $u_{0}>\tilde{p}(z_{0})$ then the sample is rejected, otherwise $u_{0}$ is retained.
        \item The corresponding $z$ values in the remaining pairs are distributed according to $p(z)$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Rejection sampling}
    \begin{figure}
        \caption{Illustration of the rejection sampling method}
        \includegraphics{Figure_4.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Rejection sampling}
    Let's verify the correctness of the rejection sampling method. Suppose that random variable $Z$ is distributed according to $q(z)$, and random variable $U$ is uniformly distributed over $[0,kq(Z)]$. We want to calculate the probability density function of the random variable $Z|0\le{}U\le\tilde{p}(Z)$:
    \begin{align*}
        P(Z\in{}E|0\le{}U\le\tilde{p}(Z))&=\frac{P(Z\in{}E,0\le{}U\le\tilde{p}(Z))}{P(0\le{}U\le\tilde{p}(Z))} \\
        &=\frac{\int_{E}q(z)\frac{\tilde{p}(z)}{kq(z)}\mathrm{d}z}{\int_{\mathbb{R}}q(z)\frac{\tilde{p}(z)}{kq(z)}\mathrm{d}z} \\
        &=\int_{E}p(z)\mathrm{d}z
    \end{align*}
    We see that the random variable $Z|0\le{}U\le\tilde{p}(Z)$ is indeed distributed according to $p(z)$.
\end{frame}

\begin{frame}
    \frametitle{Rejection sampling}
    Let's calculate the probability that a sample will be accepted:
    \begin{equation*}
        P_{\textrm{accept}}=\int_{\mathbb{R}}q(z)\frac{\tilde{p}(z)}{kq(z)}\mathrm{d}z=\frac{Z_{p}}{k}
    \end{equation*}
    We see that the constant $k$ should be as small as possible subject to the limitation that $kq(z)$ must be nowhere less than $\tilde{p}(z)$.
\end{frame}

\begin{frame}
    \frametitle{Adaptive rejection sampling}
    \begin{itemize}
        \item In many instances, it can be difficult to determine a suitable analytic form for the envelope distribution $q(z)$.
        \item An alternative approach is to construct the envelope function on the fly based on measured values of the distribution $p(z)$.
        \item Constructing an envelope function is particularly straightforward when $p(z)$ is log concave.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Adaptive rejection sampling}
    \begin{itemize}
        \item Evaluate the function $\log{}p(z)$ and its gradient at some initial set of grid points.
        \item The intersections of the resulting tangent lines are used to construct the envelope function.
        \item Draw a sample value from the envelope distribution. This is straightforward because the envelope function comprises a piecewise exponential distribution.
        \item If the sample is accepted, then it will be a draw from the desired distribution.
        \item If the sample is rejected, then it is incorporated into the set of grid points, a new tangent line is computed, and the envelope function is thereby refined.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Adaptive rejection sampling}
    \begin{figure}
        \caption{Illustration of the construction of an envelope function for adaptive rejection sampling}
        \includegraphics{Figure_6.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Importance sampling}
    The technique of importance sampling provides a framework for approximating expectations directly but does not itself provide a mechanism for drawing samples from a distribution $p(z)$.
    \bigbreak
    Suppose we wish to calculate the expectation of a function $f(z)$ with respect to a distribution $p(z)$:
    \begin{itemize}
        \item The distribution $p(z)$ can be evaluated only up to a normalization constant, so that $p(z)=\frac{\tilde{p}(z)}{Z_{p}}$, where $Z_{p}$ is unknown.
        \item Because it is difficult to draw samples directly from $p(z)$, we rely on a proposal distribution $q(z)$ from which it is easy to draw samples.
        \item The distribution $q(z)$ also has an unknown normalization constant $Z_{q}$, so that $q(z)=\frac{\tilde{q}(z)}{Z_{q}}$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Importance sampling}
    Let's calculate the expectation of $f(z)$ with respect to $p(z)$:
    \begin{align*}
        E(f)&=\int{}f(z)p(z)\mathrm{d}z=\int{}f(z)\frac{p(z)}{q(z)}q(z)\mathrm{d}z \\
        &\approx\frac{1}{L}\sum_{l=1}^{L}f(z^{(l)})\frac{p(z^{(l)})}{q(z^{(l)})}=\frac{Z_{q}}{Z_{p}}\frac{1}{L}\sum_{l=1}^{L}f(z^{(l)})\frac{\tilde{p}(z^{(l)})}{\tilde{q}(z^{(l)})} \\
        \frac{Z_{p}}{Z_{q}}&=\frac{1}{Z_{q}}\int\tilde{p}(z)\mathrm{d}z=\frac{1}{Z_{q}}\int\frac{\tilde{p}(z)}{q(z)}q(z)\mathrm{d}z \\
        &\approx\frac{1}{Z_{q}}\frac{1}{L}\sum_{l=1}^{L}\frac{\tilde{p}(z^{(l)})}{q(z^{(l)})}=\frac{1}{L}\sum_{l=1}^{L}\frac{\tilde{p}(z^{(l)})}{\tilde{q}(z^{(l)})}
    \end{align*}
    where the samples $\{z^{(l)}\}$ are drawn from $q(z)$.
\end{frame}

\begin{frame}
    \frametitle{Importance sampling}
    Let:
    \begin{align*}
        \tilde{r}_{l}&=\frac{\tilde{p}(z^{(l)})}{\tilde{q}(z^{(l)})} \\
        w_{l}&=\frac{\tilde{r}_{l}}{\sum_{l'}\tilde{r}_{l'}}
    \end{align*}
    we see that:
    \begin{equation*}
        E(f)\approx\frac{\sum_{l=1}^{L}f(z^{(l)})\tilde{r}_{l}}{\sum_{l=1}^{L}\tilde{r}_{l}}=\sum_{l=1}^{L}w_{l}f(z^{(l)})
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Sampling-importance-resampling}
    \begin{itemize}
        \item Draw $L$ samples $z^{(1)},\hdots,z^{(L)}$ from $q(z)$.
        \item Construct weights $w_{1},\hdots,w_{L}$ using $w_{l}=\frac{\tilde{r}_{l}}{\sum_{l'}\tilde{r}_{l'}}=\frac{\tilde{p}(z^{(l)})/q(z^{(l)})}{\sum_{l'}\tilde{p}(z^{(l')})/q(z^{(l')})}$.
        \item Draw $L$ samples from the discrete distribution $(z^{(1)},\hdots,z^{(L)})$ with probabilities given by the weights $(w_{1},\hdots,w_{L})$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Sampling-importance-resampling}
    Let's verify the correctness of the sampling-importance-resampling method.
    \begin{equation*}
        P(z\le{}a)=\sum_{l=1}^{L}I(z^{(l)}\le{}a)w_{l}=\frac{\sum_{l=1}^{L}I(z^{(l)}\le{}a)\frac{\tilde{p}(z^{(l)})}{q(z^{(l)})}}{\sum_{l=1}^{L}\frac{\tilde{p}(z^{(l)})}{q(z^{(l)})}}
    \end{equation*}
    where $I$ is the indicator function. Taking the limit $L\to\infty$:
    \begin{align*}
        P(z\le{}a)&=\frac{\int{}I(z\le{}a)\frac{\tilde{p}(z)}{q(z)}q(z)\mathrm{d}z}{\int\frac{\tilde{p}(z)}{q(z)}q(z)\mathrm{d}z}=\frac{\int_{-\infty}^{a}\tilde{p}(z)\mathrm{d}z}{\int_{-\infty}^{\infty}\tilde{p}(z)\mathrm{d}z}=\int_{-\infty}^{a}p(z)\mathrm{d}z
    \end{align*}
\end{frame}

\section{Markov Chain Monte Carlo}

\begin{frame}
    \frametitle{Markov Chain Monte Carlo}
    Suppose that:
    \begin{itemize}
        \item We wish to sample from a distribution $p(z)$, and sampling directly from $p(z)$ is difficult.
        \item We are easily able to evaluate $p(z)$ for any given value of $z$, up to some normalizing constant $Z_{p}$, so that $p(z)=\frac{1}{Z_{p}}\tilde{p}(z)$, where $\tilde{p}(z)$ can readily be evaluated, but $Z_{p}$ is unknown.
        \item We maintain a record of the current state $z^{(\tau)}$, and the proposal distribution $q(z|z^{(\tau)})$ is conditioned on this current state.
        \item At each cycle of the algorithm, we generate a candidate sample $z^{*}$ from the proposal distribution and then accept the sample according to an appropriate criterion.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The Metropolis algorithm}
    In the basic Metropolis algorithm, we assume that the proposal distribution is symmetric, that is $q(z_{A}|z_{B})=q(z_{B}|z_{A})$ for all values of $z_{A}$ and $z_{B}$. The candidate sample is then accepted with probability:
    \begin{equation*}
        A(z^{*},z^{(\tau)})=\min(1,\frac{\tilde{p}(z^{*})}{\tilde{p}(z^{(\tau)})})
    \end{equation*}
    As we will see, if $q(z_{A}|z_{B})$ is positive for any values of $z_{A}$ and $z_{B}$, the distribution of $z^{(\tau)}$ tends to $p(z)$ as $\tau\to\infty$.
\end{frame}

\begin{frame}
    \frametitle{The Metropolis algorithm}
    \begin{algorithm}[H]
        \caption{Metropolis sampling}
        $z_{\mathrm{prev}}\gets{}z^{(0)}$\;
        \For{$\tau\gets{}1$ \KwTo $T$}{
            $z^{*}\sim{}q(z|z_{\mathrm{prev}})$\;
            $u\sim\mathcal{U}(0,1)$\;
            \If{$\frac{\tilde{p}(z^{*})}{\tilde{p}(z_{\mathrm{prev}})}>u$}{
                $z_{\mathrm{prev}}\gets{}z^{*}$\;
            }
        }
        \Return{$z_{\mathrm{prev}}$}\;
    \end{algorithm}
\end{frame}

\begin{frame}
    \frametitle{Markov chains}
    For a first-order Markov chain $z^{(1)},\hdots,z^{(M)},\hdots$:
    \begin{itemize}
        \item The transition probability $T_{m}(z^{(m)},z^{(m+1)})$ from $z^{(m)}$ to $z^{(m+1)}$ is defined as $p(z^{(m+1)}|z^{(m)})$.
        \item A Markov chain is called homogeneous if the transition probabilities are the same for all $m$.
        \item A distribution is said to be invariant with respect to a Markov chain if the marginal distributions $p(z^{(m)})$ are invariant.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Markov chains}
    A transition probability $T(z,z')$ is said to satisfy detailed balance with respect to a distribution $p(z)$ if:
    \begin{equation*}
        p(z)T(z,z')=p(z')T(z',z)
    \end{equation*}
    It is easily seen that $p(z)$ is invariant:
    \begin{equation*}
        \int{}p(z)T(z,z')\mathrm{d}z=\int{}p(z')T(z',z)\mathrm{d}z=p(z')\int{}p(z|z')\mathrm{d}z=p(z')
    \end{equation*}
    A Markov chain that respects detailed balance is said to be reversible.
\end{frame}

\begin{frame}
    \frametitle{Markov chains}
    Our goal is to use Markov chains to sample from a given distribution $p^{*}(z)$:
    \begin{itemize}
        \item We can achieve this if we set up a Markov chain such that $p^{*}(z)$ is invariant.
        \item We must also require that for $m\to\infty$, the distribution $p(z^{(m)})$ converges to $p^{*}(z)$, irrespective of the choice of initial distribution $p(z^{(0)})$. This property is called ergodicity, and the invariant distribution is then called the equilibrium distribution.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The Metropolis-Hastings algorithm}
    The Metropolis-Hastings algorithm, which is a generalization of the basic Metropolis algorithm, applies when the proposal distribution is no longer a symmetric function of its arguments. The candidate sample is accepted with probability:
    \begin{equation*}
        A_{k}(z^{*},z^{(\tau)})=\min(1,\frac{\tilde{p}(z^{*})q_{k}(z^{(\tau)}|z^{*})}{\tilde{p}(z^{(\tau)})q_{k}(z^{*}|z^{(\tau)})})
    \end{equation*}
    Here $k$ labels the members of the set of possible transitions being considered. For a symmetric proposal distribution, the Metropolis-Hastings criterion reduces to the standard Metropolis criterion.
\end{frame}

\begin{frame}
    \frametitle{The Metropolis-Hastings algorithm}
    \begin{algorithm}[H]
        \caption{Metropolis-Hastings sampling}
        $z_{\mathrm{prev}}\gets{}z^{(0)}$\;
        \For{$\tau\gets{}1$ \KwTo $T$}{
            $k\gets{}M(\tau)$\;
            $z^{*}\sim{}q_{k}(z|z_{\mathrm{prev}})$\;
            $u\sim\mathcal{U}(0,1)$\;
            \If{$\frac{\tilde{p}(z^{*})q_{k}(z_{\mathrm{prev}}|z^{*})}{\tilde{p}(z_{\mathrm{prev}})q_{k}(z^{*}|z_{\mathrm{prev}})}>u$}{
                $z_{\mathrm{prev}}\gets{}z^{*}$\;
            }
        }
        \Return{$z_{\mathrm{prev}}$}\;
    \end{algorithm}
\end{frame}

\begin{frame}
    \frametitle{The Metropolis-Hastings algorithm}
    We can show that $p(z)$ is an invariant distribution of the Markov chain defined by the Metropolis-Hastings algorithm by showing that detailed balance is satisfied:
    \begin{align*}
        p(z)T_{k}(z,z')&=p(z)q_{k}(z'|z)A_{k}(z',z) \\
        &=\min(p(z)q_{k}(z'|z),p(z')q_{k}(z|z')) \\
        &=\min(p(z')q_{k}(z|z'),p(z)q_{k}(z'|z)) \\
        &=p(z')q_{k}(z|z')A_{k}(z,z') \\
        &=p(z')T_{k}(z',z)
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Gibbs sampling}
    \begin{algorithm}[H]
        \caption{Gibbs sampling}
        \For{$\tau\gets{}1$ \KwTo $T$}{
            \For{$m\gets{}1$ \KwTo $M$}{
                $z_{m}\sim{}p(z_{m}|\{z_{m'\ne{}m}\})$\;
            }
        }
        \Return{$\{z_{1},\hdots,z_{M}\}$}\;
    \end{algorithm}
\end{frame}

\begin{frame}
    \frametitle{Gibbs sampling}
    Let's first verify that the distribution $p(z)$ is an invariant of each of the Gibbs sampling steps individually. Suppose at a certain step, $z$ is transitioned to $z'$, and the variable that has been updated is $z_{m}$:
    \begin{align*}
        &\int{}T(z,z')p(z)\mathrm{d}z \\
        &=\int{}p(z_{m}'|z_{m'\ne{}m}')p(z_{1}',\hdots,z_{m-1}',z_{m},z_{m+1}',\hdots,z_{M}')\mathrm{d}z_{m} \\
        &=p(z_{m}'|z_{m'\ne{}m}')p(z_{m'\ne{}m}')=p(z')
    \end{align*}
    To ensure that the Gibbs sampling procedure samples from the correct distribution, it has to be ergodic. A sufficient condition for ergodicity is that none of the conditional distributions are anywhere zero.
\end{frame}

\begin{frame}
    \frametitle{Gibbs sampling}
    The Gibbs sampling procedure is a particular instance of the Metropolis-Hastings algorithm. Consider a Metropolis-Hastings sampling step, at which $z$ is transitioned to $z^{*}$, and the variable that has been updated is $z_{m}$. The factor that determines the acceptance probability is given by:
    \begin{align*}
        A(z^{*},z)&=\min(1,\frac{p(z^{*})q_{m}(z|z^{*})}{p(z)q_{m}(z^{*}|z)}) \\
        &=\min(1,\frac{p(z^{*}_{m}|z^{*}_{m'\ne{}m})p(z^{*}_{m'\ne{}m})p(z_{m}|z^{*}_{m'\ne{}m})}{p(z_{m}|z_{m'\ne{}m})p(z_{m'\ne{}m})p(z^{*}_{m}|z_{m'\ne{}m})}) \\
        &=1
    \end{align*}
    Thus the Metropolis-Hastings steps are always accepted.
\end{frame}

\begin{frame}
    \frametitle{Gibbs sampling}
    One approach to reducing the random walk behavior in Gibbs sampling is called over-relaxation. It applies to problems for which the conditional distributions are Gaussian. At each step of the Gibbs sampling algorithm, the conditional distribution for a particlar component $z_{m}$ has some mean $\mu_{m}$ and some variance $\sigma^{2}_{m}$. In the over-relaxation framework, the value of $z_{m}$ is replaced with:
    \begin{equation*}
        z_{m}'=\mu_{m}+\alpha_{m}(z_{m}-\mu_{m})+\sigma_{m}\sqrt{1-\alpha_{m}^{2}}\nu
    \end{equation*}
    where $\nu$ is a Gaussian random variable with zero mean and unit variance, and $\alpha$ is a parameter such that $-1<\alpha<1$. This step leaves the desired distribution invariant because $z_{m}'$ also has mean $\mu_{m}$ and variance $\sigma^{2}_{m}$. The effect of over-relaxation is to encourage directed motion through state space when the variables are highly correlated.
\end{frame}

\begin{frame}
    \frametitle{Ancestral sampling}
    For a directed graph with no observed variables, it is straightforward to sample from the joint distribution using the following ancestral sampling approach:
    \begin{equation*}
        p(z)=\prod_{m=1}^{M}p(z_{m}|\mathrm{pa}(m))
    \end{equation*}
    To obtain a sample from the joint distribution, we make one pass through the set of variables in the order $z_{1},\hdots,z_{M}$ sampling from the conditional distributions $p(z_{m}|\mathrm{pa}(m))$.
\end{frame}

\begin{frame}
    \frametitle{Ancestral sampling}
    Consider a directed graph in which some of the nodes, which comprise the evidence set $\mathcal{E}$, are instantiated with observed values. The likelihood weighted sampling method:
    \begin{itemize}
        \item For each variable in turn:
        \begin{itemize}
            \item If that variable is in the evidence set, the it is just set to its instantiated value.
            \item If it is not in the evidence set, then it is sampled from the conditional distribution $p(z_{m}|\mathrm{pa}(m))$.
        \end{itemize}
        \item The weighting associated with the resulting sample $z$ is then given by $r(z)=\prod_{z_{m}\notin\mathcal{E}}\frac{p(z_{m}|\mathrm{pa}(m))}{p(z_{m}|\mathrm{pa}(m))}\prod_{z_m\in\mathcal{E}}\frac{p(z_{m}|\mathrm{pa}(m))}{1}=\prod_{z_{m}\in\mathcal{E}}p(z_{m}|\mathrm{pa}(m))$.
    \end{itemize}
\end{frame}

\section{Langevin Sampling}

\begin{frame}
    \frametitle{Langevin sampling}
    \begin{itemize}
        \item The Metropolis-Hastings algorithm can be relatively inefficient since the proposal distribution is often a simple, fixed distribution that can generate updates in any direction in the data space, leading to a random walk.
        \item We can introduce Markov chain sampling algorithms that make use of the gradient of the probability density with respect to the data vector so as to take steps that preferentially move towards regions of higher probability.
        \item Langevin sampling avoids the use of an acceptance test, and arises in the context of machine learning models defined in terms of energy functions.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Energy-based models}
    \begin{itemize}
        \item The energy function $E(x;w)$ is a real valued function of its arguments with no other constraints.
        \item The exponential $\exp(-E(x;w))$ can be viewed as an unnormalized probability distribution over $x$:
        \begin{itemize}
            \item The introduction of the minus sign means that higher values of energy correspond to lower values of probability.
        \end{itemize}
        \item We can then define a normalized distribution using $p(x;w)=\frac{1}{Z(w)}\exp(-E(x;w))$:
        \begin{itemize}
            \item $Z(w)$, known as the partition function, is defined by $Z(w)=\int\exp(-E(x;w))\mathrm{d}x$.
        \end{itemize}
        \item The energy function is often modelled using a deep neural network with input vector $x$ and a scalar output $E(x;w)$, where $w$ represents the weights and biases in the network.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Energy-based models}
    \begin{itemize}
        \item The big advantage of energy-based models is their flexibility in that they bypass the requirement for normalization.
        \item A corresponding disadvantage is that since the normalizing constant is unknown, they can be more difficult to train:
        \begin{itemize}
            \item To compute the gradient of the log likelihood with respect to $w$, we need to know the form of $Z(w)$.
            \item However, for many choices of the energy function $E(x;w)$, it will be impractical to evaluate the partition function.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Maximizing the likelihood}
    We can make use of Monte Carlo sampling methods to approximate the gradient of the log likelihood with respect to the model parameters. For a single data point $x$:
    \begin{equation*}
        \nabla_{w}\log{}p(x;w)=-\nabla_{w}E(x;w)-\nabla_{w}\log{}Z(w)
    \end{equation*}
    Assume that the data points from the training set are drawn independently from some unknown distribution $p_{\mathcal{D}}(x)$:
    \begin{equation*}
        E_{x\sim{}p_{\mathcal{D}}(x)}(\nabla_{w}\log{}p(x;w))=-E_{x\sim{}p_{\mathcal{D}}(x)}(\nabla_{w}E(x;w))-\nabla_{w}\log{}Z(w)
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Maximizing the likelihood}
    Let's calculate $-\nabla_{w}\log{}Z(w)$:
    \begin{align*}
        -\nabla_{w}\log{}Z(w)&=-\frac{1}{Z(w)}\nabla_{w}\int\exp(-E(x;w))\mathrm{d}x \\
        &=-\frac{1}{Z(w)}\int\exp(-E(x;w))(-\nabla_{w}E(x;w))\mathrm{d}x \\
        &=\int\nabla_{w}E(x;w)p(x;w)\mathrm{d}x=E_{x\sim{}p_{\mathcal{M}}(x)}(\nabla_{w}E(x;w))
    \end{align*}
    where the model distribution $p(x;w)$ is written as $p_{\mathcal{M}}(x)$ in the expectation to make it clear. Now we have:
    \begin{align*}
        &E_{x\sim{}p_{\mathcal{D}}(x)}(\nabla_{w}\log{}p(x;w)) \\
        &=-E_{x\sim{}p_{\mathcal{D}}(x)}(\nabla_{w}E(x;w))+E_{x\sim{}p_{\mathcal{M}}(x)}(\nabla_{w}E(x;w))
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Maximizing the likelihood}
    \begin{figure}
        \caption{Illustration of the training of an energy-based model by maximizing the likelihood}
        \includegraphics[height=0.6\textheight]{Figure_13.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Langevin dynamics}
    When training an energy-based model, we need to approximate the two terms on the right-hand side. For any given value of $x$, we can evaluate $\nabla_{w}E(x;w)$ using automatic differentiation. For the first term, we can use the training data set to estimate the expectation over $x$:
    \begin{equation*}
        E_{x\sim{}p_{\mathcal{D}}(x)}(\nabla_{w}E(x;w))\approx\frac{1}{N}\sum_{n=1}^{N}\nabla_{w}E(x_{n};w)
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Langevin dynamics}
    For the second term, we need to draw samples from the model distribution. This can be done using Markov chain Monte Carlo methods. One popular approach is called Langevin sampling. We start by drawing an initial value $x^{(0)}$ from a prior distribution, and then we iterate the following Markov chain steps:
    \begin{equation*}
        x^{(\tau+1)}=x^{(\tau)}+\eta\nabla_{x}\log{}p(x^{(\tau)};w)+\sqrt{2\eta}\epsilon^{(\tau)}
    \end{equation*}
    where $\epsilon^{(\tau)}\sim\mathcal{N}(\epsilon;0,I)$ are independent samples from a zero-mean, unit-covariance Gaussian distribution, and the parameter $\eta$ controls the step size. It can be shown that, in the limits of $\eta\to{}0$ and $\tau\to\infty$, the value of $x^{(\tau)}$ is an independent sample from the distribution $p(x)$.
\end{frame}

\begin{frame}
    \frametitle{Langevin dynamics}
    One more word about $\nabla_{x}\log{}p(x;w)$, which is called the score function:
    \begin{align*}
        \nabla_{x}\log{}p(x;w)&=\frac{1}{p(x;w)}\nabla_{x}(\frac{1}{Z(w)}\exp(-E(x;w))) \\
        &=\frac{1}{p(x;w)}\frac{1}{Z(w)}\exp(-E(x;w))(-\nabla_{x}E(x;w)) \\
        &=-\nabla_{x}E(x;w)
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Langevin dynamics}
    \begin{algorithm}[H]
        \caption{Langevin sampling}
        $x\gets{}x_{0}$\;
        \For{$\tau\gets{}1$ \KwTo $T$}{
            $\epsilon\sim\mathcal{N}(\epsilon;0,I)$\;
            $x\gets{}x+\eta\nabla_{x}\log{}p(x;w)+\sqrt{2\eta}\epsilon$\;
        }
        \Return{$x$}\;
    \end{algorithm}
\end{frame}

\begin{frame}
    \frametitle{Langevin dynamics}
    We can repeat the Langevin sampling process to generate a set of samples $\{x_{1},\hdots,x_{M}\}$ from the model distribution and then approximate the second term using:
    \begin{equation*}
        E_{x\sim{}p_{\mathcal{M}}(x)}(\nabla_{w}E(x;w))\approx\frac{1}{M}\sum_{m=1}^{M}\nabla_{w}E(x_{m};w)
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Langevin dynamics}
    Running long Markov chains to generate independent samples can be computationally expensive, and so we need to consider practical approximations. One approach is called contrastive divergence:
    \begin{itemize}
        \item Running a Monte Carlo chain starting with one of the training data points $x_{n}$.
        \item Running for only a few steps of Monte Carlo, perhaps even as few as one step.
        \item The resulting sample will be far from unbiased and will lie close to the data manifold.
        \item This can prove effective for tasks such as discrimination but is expected to be less effective in learning a generative model.
    \end{itemize}
\end{frame}

\end{document}